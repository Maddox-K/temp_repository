{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17935f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import fasttext\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "147d02bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tomatoes_path = \"C:/Users/maddo/CS770_data/project_data/rotten_tomatoes_data\"\n",
    "\n",
    "df = pd.read_csv(tomatoes_path + \"/rotten_tomatoes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3173bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df['review_content'].dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4262858",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(tomatoes_path + \"/tomatoes_reviews_text.txt\", 'a', encoding=\"utf-8\")\n",
    "\n",
    "for review in df.values:\n",
    "    f.write(review + \" \")\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1e8fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_vectors = fasttext.train_unsupervised(tomatoes_path + \"/tomatoes_reviews_text.txt\")\n",
    "review_vectors.save_model(tomatoes_path + \"/tomatoes_vectors.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09293956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.load_model(tomatoes_path + \"/tomatoes_vectors.bin\")\n",
    "\n",
    "with open(tomatoes_path + \"/tomatoes_vectors.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    # Write vocabulary size and embedding dimensions as the first line\n",
    "    f.write(f\"{len(model.get_words())} {model.get_dimension()}\\n\")\n",
    "    \n",
    "    # Write each word and its vector to the file\n",
    "    for word in model.get_words():\n",
    "        vector = model.get_word_vector(word)\n",
    "        vector_str = \" \".join(map(str, vector))\n",
    "        f.write(f\"{word} {vector_str}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fc8629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_path = \"C:/Users/maddo/CS770_data/project_data/imdb_data/IMDB Dataset.csv\"\n",
    "\n",
    "df = pd.read_csv(imdb_path)\n",
    "\n",
    "df['review'] = df['review'].str.replace(\"<br /><br />\", \" \").str.replace('\\u200b', '').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d1b2971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X contains text contents of reviews, y contains labeled sentiment\n",
    "X = df['review']\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "104753ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training, testing and validation data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fdc31f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to Train and Scale Raw Text Data:\n",
      "1039.459019 Seconds\n"
     ]
    }
   ],
   "source": [
    "# record start time of data training model\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# tfidf vectorizer using character n-grams (with bounds), n-gram range of 3-6 letters, and keep words in vocab that appear at least 5 times\n",
    "vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,6), min_df=5)\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "# dimensionality reduction of sparce vectors from vectorization\n",
    "# convert sparse vectors into dense word embedding vectors\n",
    "# somewhat of an imitation of fasttext's word embedding hidden layer\n",
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "X_train_dense = svd.fit_transform(X_train_vectors)\n",
    "X_test_dense = svd.transform(X_test_vectors)\n",
    "\n",
    "def load_embeddings(file):\n",
    "    embeddings = {}\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "file = tomatoes_path + \"/tomatoes_vectors.txt\"  # Example path to 100D GloVe embeddings\n",
    "embeddings = load_embeddings(file)\n",
    "\n",
    "# Step 4: Function to compute average word embedding for each document\n",
    "def compute_average_word_embedding(text, embeddings):\n",
    "    words = text.split()\n",
    "    word_vectors = [embeddings.get(word, np.zeros(100)) for word in words]\n",
    "    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(100)\n",
    "\n",
    "# Compute average word embeddings for X_train and X_test\n",
    "X_train_embeddings = np.array([compute_average_word_embedding(text, embeddings) for text in X_train])\n",
    "X_test_embeddings = np.array([compute_average_word_embedding(text, embeddings) for text in X_test])\n",
    "\n",
    "# Step 5: Concatenate reduced TF-IDF vectors and average word embeddings\n",
    "X_train_combined = np.hstack((X_train_dense, X_train_embeddings))\n",
    "X_test_combined = np.hstack((X_test_dense, X_test_embeddings))\n",
    "\n",
    "# scale resulting data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_combined)\n",
    "X_test_scaled = scaler.transform(X_test_combined)\n",
    "\n",
    "# get total time taken to train and scale\n",
    "end_time = time.perf_counter()\n",
    "initial_elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Time Taken to Train and Scale Raw Text Data:\")\n",
    "print(str(initial_elapsed_time) + \" Seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25e91837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize time and model accuracy lists\n",
    "times = []\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7493ade9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Alpha Value:\n",
      "{'alpha': 0.01}\n",
      "\n",
      "Total Time Taken to Train and Evaluate Model:\n",
      "1060.0550573 Seconds\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8777 1212]\n",
      " [1055 8956]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.88      0.89      9989\n",
      "    positive       0.88      0.89      0.89     10011\n",
      "\n",
      "    accuracy                           0.89     20000\n",
      "   macro avg       0.89      0.89      0.89     20000\n",
      "weighted avg       0.89      0.89      0.89     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'alpha': [.00001, .0001, .001, .01, .1, 1]}\n",
    "\n",
    "# record start time of training and predicting model\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# svm with stochastic gradient descent training\n",
    "sgdlog_clf = SGDClassifier(loss='hinge', n_jobs=-1)\n",
    "\n",
    "grid_search_2 = GridSearchCV(sgdlog_clf, param_grid, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_2.fit(X_train_scaled, y_train)\n",
    "best_sgdlog = grid_search_2.best_estimator_\n",
    "\n",
    "y_pred = best_sgdlog.predict(X_test_scaled)\n",
    "\n",
    "# get total time taken to train model and predict\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "times.append(elapsed_time + initial_elapsed_time)\n",
    "\n",
    "accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"Tuned Alpha Value:\")\n",
    "print(grid_search_2.best_params_)\n",
    "\n",
    "print(\"\\nTotal Time Taken to Train and Evaluate Model:\")\n",
    "print(str(elapsed_time + initial_elapsed_time) + \" Seconds\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a80246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
